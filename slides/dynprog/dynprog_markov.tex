\documentclass[11pt,xcolor={dvipsnames},aspectratio=159,hyperref={pdftex,pdfpagemode=UseNone,hidelinks,pdfdisplaydoctitle=true},usepdftitle=false]{beamer}
\usepackage{presentation}[aspectratio=169]
\usepackage{math}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\hypersetup{
    colorlinks=magenta,
    linkcolor=magenta,
    filecolor=magenta,      
    urlcolor=magenta,
    }
% Enter title of presentation PDF:
\hypersetup{pdftitle={Markov Dynamic Programming}}


\begin{document}
% Enter presentation title:
\title{Markov Dynamic Programming}
\subtitle{Quantitative Economics 2024}
% Enter presentation information:

% Enter presentation authors:
\author{Piotr Å»och}%
% Enter presentation location and date (optional; comment line if not needed):
\frame{\titlepage}

% Fill out content of presentation:
\begin{frame}{A typical problem}   
   
\begin{itemize}
    \item The planner chooses a path of actions $\bp{A_t}_{t\geq0}$ to maximize 
    \begin{align*}
         \E_0 \sum_{t=0}^\infty \beta^t r\of{X_t,A_t} 
    \end{align*}
    where $\bp{X_t}_{t\geq0}$ is a state process ($X_0$ is given).
    \item \text{X} is a finite set: \al{state space}.
    \item \text{A} is a finite set: \al{action space}.
    \item $\Gamma$ is a \alb{correspondence} from $X$ to $A$. Intuitively: the set of actions feasible given the state.
\end{itemize}
\end{frame}

\begin{frame}{MDP}   
 \begin{itemize}
    \item Given $\text{ A and X }$ a finite \al{Markov decision process} (MDP) is a tuple $ \Mc = \bp{\Gamma,P,r,\beta}$ where
    \begin{enumerate}
        \item $\Gamma: X \to A$ is a nonempty correspondence from $\text{X}$ to $\text{A}$ defining feasible state-action pairs \begin{align*}
            \text{G} \coloneq \bc{\bp{x,a} \in \text{X} \times \text{A}: a \in \Gamma\of{x}}
        \end{align*}
        \item a \alb{stochastic kernel} $P$ from $\text{G}$ to $\text{X}$: \begin{align*}
            \sum_{x^\prime \in \text{X}} P\of{x,a,x^\prime} = 1 \text{ for all } \bp{x,a} \in \text{G}.
        \end{align*}
        \item a function $r$ from $\text{G}$ to $\R$ is a \al{reward function}
        \item $\b \in \bp{0,1}$ is a discount factor.
    \end{enumerate}
\end{itemize}
\end{frame}



\begin{frame}{MDP}   
    \begin{itemize}
        \item The Bellman equation associated with $\Mc$ is \begin{align*}
            v\of{x} = \max_{a \in \Gamma\of{x}} \bc{r\of{x,a} + \b \sum_{x^\prime \in \text{X}} P\of{x,a,x^\prime} v\of{x^\prime}} \text{ for all } x \in \text{X}.
        \end{align*}
        \item This is an equation in the unknown function $v \in \R^{\text{X}}$ ($\R^{\text{X}}$ is a set of all functions from $\text{X}$ to $R$).
        \item We will show that the solution to the Bellman equation equals to the largest possible value of the objective function in the \alb{sequence problem}:     
        \begin{align*}
           \max \E \sum_{t=0}^\infty \beta^t r\of{X_t,A_t}, \quad \text{ subject to } A_t \in \Gamma\of{X_t} \text{ for all } t \geq 0. 
        \end{align*}
    \end{itemize}
    \end{frame}

\begin{frame}{Policies}   
    \begin{itemize}
        \item Let $\Sigma$ be the set of all \al{feasible policies} given $\Mc$: \begin{align*}
            \Sigma \coloneq \bc{\sigma \in \text{A}^\text{X} : \sigma\of{x}\in\Gamma\of{x} \text{ for all } x \in \text{X}}.
        \end{align*}
        \item For any $\sigma \in \Sigma$ we have $P_\sigma$ is a stochastic kernel from $\text{X}$ to $\text{X}$: \begin{align*}
            P_\sigma\of{x,x^\prime} \coloneq P\of{x,\sigma\of{x},x^\prime} \text{ for all } \bp{x,x^\prime} \in \text{X}\quad \text{so } P_\sigma\in\Mc\of{\R^\text{X}}.
            \end{align*}
            {\color{gray} Note: notational issue - $\Mc$ here is not MDP, it is a set of Markov operators.}
            \item Similarly, for any $\sigma \in \Sigma$ we have $r_\sigma$, a function from $\text{X}$ to $\R$: \begin{align*}
            r_\sigma\of{x} \coloneq r\of{x,\sigma\of{x}} \text{ for all } x \in \text{X}\quad \text{so } r_\sigma\in\R^\text{X}.
            \end{align*}
    \end{itemize}
    \end{frame}

\begin{frame}{Policies}   
    \begin{itemize}
        \item Define $\E_{x_0}\bs{\cdot} \coloneq \E\bs{\cdot \mid X_0 = x_0}$. The \alb{lifetime value} of following $\sigma\in\Sigma$ from $x$ is \begin{align*}
            v_\sigma\of{x} \coloneq \E_{x}\bs{\sum_{t=0}^\infty \beta^t r_\sigma\of{X_t}}
        \end{align*}
      where $X_t$ is $P_\sigma$-Markov with $X_0 = x$.
      \item Since $\beta \in \bp{0,1}$, we can calculate \begin{align*}
        v_\sigma\of{x} = \sum_{t=0}^\infty \beta^t P^t_\sigma r_\sigma = \bp{I - \beta P_\sigma}^{-1} r_\sigma. 
      \end{align*}
    \end{itemize}
    \end{frame}

\begin{frame}{Policy operator}   
    \begin{itemize}
        \item Define the \alg{policy operator} $T_\sigma$:\begin{align*}
            \bp{T_\sigma v}\of{x} \coloneq r\of{x,\sigma\of{x}} + \b \sum_{x^\prime \in \text{X}} v\of{x^\prime} P\of{x,\sigma\of{x},x^\prime} \text{ for all } x \in \text{X}.
        \end{align*}
        \item We denote a fixed point of $T_\sigma$ by $v_\sigma$.
        \item We will now prove $T_\sigma$ is a contraction of modulus $\beta$ on $\R^\text{X}$ under norm $\norm{\cdot}_\infty$.
        \item We will also show that $T_\sigma$ is \alb{order-preserving}: if $v \leq w$ then $T_\sigma v \leq T_\sigma w$.
    \end{itemize}
    \end{frame}

\begin{frame}{Policy operator}   
    \begin{itemize}
        \item Take any $v,w \in \R^\text{X}$ and $\sigma \in \Sigma$.
        \item Fix $x\in\text{X}$. We have \begin{align*}
            \abs{\bp{T_\sigma v}\of{x} - \bp{T_\sigma w}\of{x}} &= \b \abs{\sum_{x^\prime \in \text{X}} \bp{v\of{x^\prime}-w\of{x^\prime}} P\of{x,\sigma\of{x},x^\prime}} \\
            & \leq \b \sum_{x^\prime \in \text{X}} \abs{v\of{x^\prime}-w\of{x^\prime}} P\of{x,\sigma\of{x},x^\prime} \\
            & \leq \b \norm{v-w}_\infty 
        \end{align*}
        \item Since it is true regardless of $x$, we have \begin{align*}
            \norm{T_\sigma v - T_\sigma w}_\infty \leq \b \norm{v-w}_\infty.
        \end{align*}
    \end{itemize}
    \end{frame}

\begin{frame}{Policy operator}   
    \begin{itemize}
        \item To show that it is order preserving take any $v,w \in \R^\text{X}$ and $\sigma \in \Sigma$.
        \item $v \leq w$ implies $P_\sigma v \leq P_\sigma w$. We can write 
        \begin{align*}
            Tv = r_\sigma + \b P_\sigma v \text{ and } Tw = r_\sigma + \b P_\sigma w.
        \end{align*}
        so $Tv \leq Tw$.
    \end{itemize}
    \end{frame}


        
\begin{frame}{Greedy policies}   
    \begin{itemize}
        \item Given MDP $\Mc$ the \alb{value function} is \begin{align*}
            v^*\of{x} \coloneq \max_{\sigma \in \Sigma} v_\sigma\of{x} \text{ for all } x \in \text{X}.
        \end{align*}
        \item We call a policy $\sigma\in\Sigma$ \alg{optimal} if $v_\sigma = v^*$. 
        \item We call a policy \alr{$v$-greedy} if \begin{align*}
            \sigma\of{x} \in \argmax_{a \in \Gamma\of{x}} \bc{r\of{x,a} + \b \sum_{x^\prime \in \text{X}} v\of{x^\prime} P\of{x,a,x^\prime} } \text{ for all } x \in \text{X}.
        \end{align*}
    \end{itemize}
    \end{frame}
 
\begin{frame}{Bellman}   
    \begin{itemize}
        \item We say that \alb{Bellman's principle of optimality} holds for MDP $\Mc$ if \begin{align*}
            \sigma\in\Sigma \text{ is optimal for } \Mc \iff \sigma \text{ is } v^*\text{-greedy}.
        \end{align*}
       \item The \alb{Bellman operator} corresponding to $\Mc$ is a self-map $T$ on $\R^\text{X}$ defined by \begin{align*}
           Tv\of{x} \coloneq \max_{a \in \Gamma\of{x}} \bc{r\of{x,a} + \b \sum_{x^\prime \in \text{X}} v\of{x^\prime} P\of{x,a,x^\prime} } \text{ for all } x \in \text{X}.
       \end{align*}
        
    \end{itemize}
    \end{frame}
            
\begin{frame}{Optimality}   
    \begin{theorem}
        Let $\Mc$ be an MDP with Bellman operator $T$. Then \begin{enumerate}
            \item $v^*$ is the unique solution to the Bellman equation $
                v = Tv$ in $\R^\text{X}$,
            \item $\lim_{k\rightarrow\infty} T^k v = v^*$ for all $v \in \R^\text{X}$,
            \item Bellman's principle of optimality holds for $\Mc$,
            \item at least one optimal policy exists.
        \end{enumerate}
    \end{theorem}
    \end{frame}
            
\begin{frame}{Optimality}   
    \begin{itemize}
       \item Instead of solving the (possibly hard) \alr{sequence problem} we can solve the (possibly easier) \alg{functional equation} $v = Tv$.
       \item Finding $v$-greedy policies is easier than looking at the entire set of feasible policies $\Sigma$.
       \item The required conditions are pretty weak. Important and somewhat hidden: sets are finite and $r: \text{G} \to \R$. 
        \end{itemize}
    \end{frame}   

\begin{frame}{Optimality}   
    \begin{itemize}
        \item We will prove (1) and (2). 
        \item Two parts of the proof:
        \begin{enumerate}
            \item Show there exists the unique fixed point of $T$.
            \item Show that the fixed point is $v^*$.
        \end{enumerate}
        \end{itemize}
    \end{frame}   

\begin{frame}{Optimality}   
    \begin{itemize}
        \item Fix $v, w$ in $\R^\text{X}$. We have \begin{align*}
            \abs{\bp{T v}\of{x} - \bp{T w}\of{x}} &= \abs{\max_{\sigma\in\Sigma} \bp{T_\sigma v }\of{x} - \max_{\sigma\in\Sigma} \bp{T_\sigma w }\of{x}} \\
            & \leq \max_{\sigma\in\Sigma} \abs{{\bp{T_\sigma v }}\of{x} - \bp{T_\sigma w }\of{x}} \\
            & = \norm{T_\sigma v-T_\sigma w}_\infty
        \end{align*}
        \item We have $\norm{Tv-Tw}_\infty \leq \norm{T_\sigma v-T_\sigma w}_\infty$ for all $\sigma \in \Sigma$.
        \item We showed earlier that $T_\sigma$ is a contraction: $\norm{T_\sigma v-T_\sigma w}_\infty \leq \b \norm{v-w}_\infty$.
        \item We thus have \begin{align*}
            \norm{Tv-Tw}_\infty \leq \b \norm{v-w}_\infty \text{ for all } v,w \in \R^\text{X}.
        \end{align*}

        \end{itemize}
    \end{frame}   

\begin{frame}{Optimality}   
    \begin{itemize}
        \item By the \alb{Banach fixed point theorem} $T$ has a unique fixed point $\bar{v}$.
       \item We will now show that $\bar{v} = v^*$.
       \item Pick $\sigma\in\Sigma$ that is $\bar{v}$-greedy. By definition we have $T_\sigma \bar{v} = \bar{v} = T \bar{v}$. So $\bar{v}$ is a fixed point of $T_\sigma$. Because we defined $v^*$ as $\max_{\sigma\in\Sigma} v_\sigma$ We have $\bar{v} \leq v^*$.
       \item Pick any $\sigma\in\Sigma$, We must have $T_\sigma v \leq Tv$ for any $v$. We know that $T_\sigma$ is order preserving, so it must be that $v_\sigma \leq \bar{v}$. This is true for any $\sigma$, so $v^* \leq \bar{v}$.
        \end{itemize}
    \end{frame}  

\begin{frame}{Optimality}   
    \begin{itemize}
        \item We can use $T_\sigma$ to look for the value function (instead of value function iteration).
        \item Start with a guess $v_0$, find a greedy policy $\sigma_0$ and calculate the fixed point of $T_{\sigma_0}$:
         \begin{align*}
            v_{\sigma_0}=\bp{I - \beta P_{\sigma_0}}^{-1} r_{\sigma_0}.
        \end{align*}
        \item Repeat the process with $v_{\sigma_0}$ -- find a greedy policy and calculate the new fixed point.
        \item Do it until convergence.
        \item This algorithm is known as \alg{policy iteration} or \alg{Howard's policy iteration}
    \end{itemize}
    \end{frame}  

\begin{frame}{HPI}
    \begin{algorithm}[H]
        \caption{Howard's Policy Iteration}
        \begin{algorithmic}[1]
        \Procedure{HPI}{} \
        \State $k \gets 1$, $\e \gets \tau + 1$, $v_k \gets v_{\text{init}}$
        \While{$\e>\tau$} \
        \State $\sigma_k \gets$ $v_k-$greedy policy
        \State $v_{k+1}=\bp{I - \beta P_{\sigma_k}}^{-1} r_{\sigma_k}$
        \State $\epsilon \gets \norm{v_{k+1}-v_k}_\infty$, $k \gets k+1$
        \EndWhile
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
\end{frame}


\begin{frame}{Example}   
    \begin{itemize}
        \item HPI converges at a faster rate than VFI.
        \item In a finite state setting, the
        algorithm always converges to an exact optimal policy in a finite number of steps, regardless of the initial condition.
        \item Drawback: computing $v_\sigma$ can be expensive.
    \end{itemize}
    \end{frame}  

\begin{frame}{Optimistic policy iteration}   
    \begin{itemize}
        \item This is a variant of HPI.
        \item Key difference: do not compute $v_\sigma$ exactly.
        \item Instead, apply the policy operator $T_\sigma$ to $v_k$ for a fixed number of iterations, $m$.
        \item For $m\rightarrow\infty$ we have HPI; for $m=1$ we have VFI.
        \item Often outperforms HPI and VFI, but this requires choosing $m$.
    \end{itemize}
    \end{frame}  

\begin{frame}{OPI}
    \begin{algorithm}[H]
        \caption{Optimistic Policy Iteration}
        \begin{algorithmic}[1]
        \Procedure{OPI}{} \
        \State $k \gets 1$, $\e \gets \tau + 1$, $v_k \gets v_{\text{init}}$
        \While{$\e>\tau$} \
        \State $\sigma_k \gets$ $v_k-$greedy policy
        \State $v_{k+1}=T^m_{\sigma_k} v_k$
        \State $\epsilon \gets \norm{v_{k+1}-v_k}_\infty$, $k \gets k+1$
        \EndWhile
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
\end{frame}

\end{document}