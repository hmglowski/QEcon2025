\documentclass[11pt,xcolor={dvipsnames},aspectratio=159,hyperref={pdftex,pdfpagemode=UseNone,hidelinks,pdfdisplaydoctitle=true},usepdftitle=false]{beamer}
\usepackage{presentation}[aspectratio=169]
\usepackage{math}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\hypersetup{
    colorlinks=magenta,
    linkcolor=magenta,
    filecolor=magenta,      
    urlcolor=magenta,
    }
% Enter title of presentation PDF:
\hypersetup{pdftitle={Intro to Dynamic Programming}}


\begin{document}
% Enter presentation title:
\title{Intro to Dynamic Programming}
\subtitle{Quantitative Economics 2024}
% Enter presentation information:

% Enter presentation authors:
\author{Piotr Å»och}%
% Enter presentation location and date (optional; comment line if not needed):
\frame{\titlepage}

% Fill out content of presentation:
\begin{frame}{A typical problem}   

\begin{itemize}    
   \item  Many problems seen in economics have a common structure:
\begin{itemize}
    \item We observe the current \alr{state} $X_t$ at time $t$.
    \item We choose an \alb{action} $A_t$ at time $t$.
    \item We get a \alg{reward} $R_{t}$ at time $t$.
    \item The state progresses to $X_{t+1}$ at time $t+1$.
\end{itemize}
\item If the largest possible $t$ is $T<\infty$, then we have a \alr{finite horizon} problem. Otherwise we have an \alb{infinite horizon} problem.
\end{itemize}
\end{frame}

\begin{frame}{Example}   
    \begin{itemize}
    \item  You saw a simple lifecycle model with known income path \emph{(perfect foresight)}.
    \item  It was easy to solve: just calculate optimal consumption and savings in every period. Possible to do it by hand. 
    \item  What if income earned by households is uncertain? 
    \item  What if households can invest in assets with uncertain returns? 
    \item  We need to find optimal solution for all possible paths of income and returns. 
\end{itemize}
\end{frame}

\begin{frame}{Example}   
    \begin{itemize}
    \item Consider a problem of a firm that produces a good. The firm wants to maximize the expected present discounted value of profits:
    \begin{align*}
         \E \sum_{t=0}^\infty \bp{\frac{1}{1+r}} \pi_t \end{align*} \begin{itemize} \item  $X_t$ is the current state of the firm. It can be the current level of capital, the current level of inventory, prices set by competitors...
    \item  $A_t$ is the action taken by the firm. It can be the level of production, the level of future inventory, the price of the good...
    \item  $R_t$ is the reward. Here it is the profit of the firm, $\pi_t$.
    \item  $X_{t+1}$ is the state of the firm in the next period. It can depend on the current state $X_t$ and the action $A_t$ taken.
\end{itemize}
\end{itemize}
\end{frame}
 

\begin{frame}{Example}   
    \begin{itemize}
    \item  This is potentially an \al{extremely} complicated problem. 
    \item  For example: the state $X_t$ can include the demand for the good -- and it could be random.
    \item  Find actions for all possible future states... 
    \item  We will learn tools that can help us solve such problems.
\end{itemize}
\end{frame}

\begin{frame}{Plan}   
    \begin{itemize}
    \item  Today we will study an example: \al{McCall's job search model (1970)}. 
    \item  Exposition based on Stachurski and Sargent (2023).
    \item  Next time: more general theory of dynamic programming.
\end{itemize}
\end{frame}

\begin{frame}
    \heading{McCall's job search model}
    \end{frame}


\begin{frame}{Two-period problem}   
    \begin{itemize}
    \item  An unemployed agent receives a job offer at wage $W_t$.
    \item  She can either \alb{accept} the offer or \alr{reject} it.
    \item  If she \alb{accepts}, she gets this wage permenantly.
    \item  If she \alr{rejects}, she gets unemployment benefit $c$.
    \item  Wage offers are independent and identically distributed (i.i.d.) and nonnegative, with distribution $\phi$: 
        \begin{itemize}
            \item $\text{W} \subset \R_+$ is a finite set of possible wages.
            \item $\phi: \text{W} \to [0,1]$ is a probability mass function, $\phi\of{w}$ is the probability of getting a wage $w$.
        \end{itemize}
    \item The agent is risk-neutral and impatient: the utlity of getting $y$ tomorrow is $\b y$, with $\b \in (0,1)$.
\end{itemize}
\end{frame}

\begin{frame}{Two-period problem}   
    \begin{itemize}
    \item The agent lives for two periods ($t=\bc{1,2}$) and starts unemployed.
    \item The question: is it better to accept a received offer or wait until tomorrow hoping for better offer?
    \item What is the lowest wage that the agent should accept?
    \item We will start analyzing the problem by looking at the second period, $t=2$: \al{backward induction}.
    \end{itemize}
\end{frame}

\begin{frame}{Period t=2}
    \begin{itemize}
    \item Suppose the agent is unemployed at $t=2$.
    \item She gets a wage offer $W_2$ which she can either \alb{accept} or \alr{reject} the offer. 
    \item \alb{Accept}: get income $W_2 \rightarrow$ get utility $W_2$.
    \item \alr{Reject}: get income $c \rightarrow$ get utility $c$.
    \item Since this is the last period of her life, she will accept if and only if $$W_2 \geq c.$$
   \end{itemize}
\end{frame}

\begin{frame}{Period t=1}
    \begin{itemize}
    \item The agent gets a wage offer $W_1$. 
    \item She can either \alb{(a) accept and get $W_1$ forever}, or \alr{(b) reject and get $c$ in period $t=1$ and then get $\max\bc{W_2,c}$ in period $t=2$}.
    \item The utility of \alb{(a)} is $W_1 + \b W_1$. We call it the \alb{stopping value}.
    \item The utility of \alr{(b)} is $h_1 \coloneq c + \b \E \max \bc{W_2,c}$. We call it the \alr{continuation value}. \begin{align*}
        h_1 = c + \b \sum_{w^\prime \in \text{W}} v_2\of{w^\prime} \phi\of{w^\prime}, \quad v_2\of{w^\prime} \coloneq \max \bc{w^\prime,c}
    \end{align*} 
    \item The agent will accept if and only if the \alb{stopping value} is greater than the \alr{continuation value}: $$W_1 + \b W_1 \geq h_1.$$
    \end{itemize}
\end{frame}

\begin{frame}{Value function}
    \begin{itemize}
    \item The key object in dynamic programming is the \al{value function}. 
    \item It is a \alg{function} that maps the state to the \alg{maximum} expected present discounted value of future rewards.
    \item In our example, there are two statges: time $t$ and the received wage offer $w$.
    \item $v_2\of{w}$ is the value function at time $t=2$ and wage $w$: the largest possible reward that the agent can get if she starts unemployed at $t=2$ and gets a wage offer $w$. We have \begin{align*} v_2\of{w} = \max \bc{w,c}.\end{align*}
    \item The time 1 value function is \begin{align*}
        v_1\of{w} \coloneq \max \bc{w + \b w, c + \b \sum_{w^\prime \in \text{W}} v_2\of{w^\prime} \phi\of{w^\prime}}.
    \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Two-period example}
    \begin{itemize}
    \item This particular problem is easy to solve.
    \item Accept if \begin{align*}
        w \geq \frac{h_1}{1+\b} 
    \end{align*}
    so the value function is \begin{align*}
        v_1\of{w} = \begin{cases}
            \bp{1+\b}w  & \text{if } w \geq \frac{h_1}{1+\b} \\
            h_1 & \text{otherwise}.
        \end{cases}
     \end{align*}
     \item In context of this example, we call $w^* \coloneq \frac{h_1}{1+\b}$ the \al{reservation wage}.
     \item We see that since $h_1$ is increasing in $c$, the reservation wage is higher when the unemployment benefit $c$ is higher.
\end{itemize}
\end{frame}


\begin{frame}{Three-period example}
    \begin{itemize}
    \item Extend the model by one period, $t=0$. 
    \item The value function at $t=0$ is \begin{align*}
        v_0\of{w} \coloneq \max \bc{w + \b w + \b^2 w, c + \b \sum_{w^\prime \in \text{W}} v_1\of{w^\prime} \phi\of{w^\prime}}.
    \end{align*}
    where the formula for $v_1$ is from the previous slide. 
    \item \al{Key insight}: at $t=0$ it is like a two-period problem.
    \item \al{All} information about the future is summarized in  $v_1$, the value function at $t=1$,.
    \item This is the standard approach: convert a complicated dynamic optimization problem into a sequence of two-period problems.
\end{itemize}
\end{frame}

\begin{frame}{Bellman equation}
    \begin{itemize}
    \item Recall we had \begin{align*}
        v_2\of{w} &= \max \bc{w,c} \\
        v_1\of{w} &= \max \bc{w + \b w, c + \b \sum_{w^\prime \in \text{W}} v_2\of{w^\prime} \phi\of{w^\prime}} \\
        v_0\of{w} &= \max \bc{w + \b w + \b^2 w, c + \b \sum_{w^\prime \in \text{W}} v_1\of{w^\prime} \phi\of{w^\prime}}.
    \end{align*}
    \item The recursive relationships between the value functions are called the \alb{Bellman equations}.
    \item \alr{Warning}: these equations are (in general) \alg{functional equations}. We need to find functions, not numbers. Here it is easy: we have a finite set of possible wages -- treat functions as vectors.
\end{itemize}
\end{frame}

\begin{frame}{Infinite horizon}
    \begin{itemize}
    \item The three-period problem is also easy to solve. 
    \item In fact, we can use the same approach (backward induction) as before for any finite horizon problem.
    \item What if the horizon is \alb{infinite}? We no longer have the \al{terminal} period. 
    \item Dynamic programming makes this problem \al{tractable}. 
\end{itemize}
\end{frame}

\begin{frame}{Infinite horizon}
    \begin{itemize}
    \item The objective function is \begin{align*}
        \E \sum_{t=0}^\infty \b^t R_t,
    \end{align*} where $R_t \in \bc{c,W_t}$.
    \item Let $\b \in \bp{0,1}$ be the discount factor, as before we let $c>0$.
    \item The wage process satisfies $\bp{W_t}_{t\geq0} \iid \phi$ where $\phi\in \Dc\of{\text{W}}$ and $\text{W} \subset \R_+$ with $\abs{\text{W}} < \infty$.
    \item For any finite or countable set $\text{F}$, $\Dc\of{\text{F}}$ is the set of distributions on $\text{F}$.
\end{itemize}
\end{frame}

\begin{frame}{Infinite horizon}
    \begin{itemize}
    \item What is \alb{stopping value}? 
    \item If the worker accepts wage $w$ she gets \begin{align*}
    w + \b w + \b^2 w + \b^3 w + \cdots = \frac{w}{1-\b}.
    \end{align*}
    \item What is the \alr{continuation value}?
    \item If the worker rejects wage $w$ she gets 
    \begin{align*}
     c + \b \sum_{w^\prime \in \text{W}} v\of{w^\prime} \phi\of{w^\prime}.
    \end{align*}
    note that the value function is the same in all periods -- there is always infinite remaining future.
\end{itemize}
\end{frame}

\begin{frame}{Infinite horizon}
    \begin{itemize}
        \item Bellman equation is:
    \begin{align*}
        v\of{w} = \max \bc{\frac{w}{1-\b}, c + \b \sum_{w^\prime \in \text{W}} v\of{w^\prime} \phi\of{w^\prime}}.
    \end{align*}
    \item \al{Principle of optimality, Bellman (1960)}: \emph{An optimal policy has the property that whatever the initial state and the initial decisions it must constitute an optimal policy with regards to the state resulting from the first decision.
    } 
    \item This is not that trivial, we will return to it (and prove it!) later. 
    \item Intepretation: value function satisfies the Bellman equation.
\end{itemize}
\end{frame}

\begin{frame}{Challenge}
    \begin{itemize}
        \item Bellman equation is:
    \begin{align*}
        v\of{w} = \max \bc{\frac{w}{1-\b}, c + \b \sum_{w^\prime \in \text{W}} v\of{w^\prime} \phi\of{w^\prime}}.
    \end{align*}
    \item Once we have $v\of{w}$ we can characterize the optimal choice of the agent.
    \item \al{Q}: how to find $v\of{w}$? It is a function!
    \item \al{Q}: is there a solution?
    \item \al{Q}: is the solution unique?
    \item \al{Q}: what are the properties of the solution?
    \item \alg{A}: we will learn how to answer these questions. 
\end{itemize}
\end{frame}

\begin{frame}{Approach I}
    \begin{itemize}
        \item This particular problem is relatively easy. 
        \item Recall how we defined the \alr{continuation value}: \begin{align*}
            h^* \coloneq c + \b \sum_{w^\prime \in \text{W}} \max \bc{\frac{w^\prime}{1-\b},h^*} \phi\of{w^\prime}
        \end{align*}
    \item We can write the value function as 
    \begin{align*}
        v\of{w} = \max \bc{\frac{w}{1-\b}, h^*}.
    \end{align*}
    \item \al{Key}: $h^*$ is a scalar, not a function. When would is break down?
    \item Find $h^*$ directly, by solving the equation numerically. 
    \item Then use $h^*$ to get the value function.     
\end{itemize}
\end{frame}

\begin{frame}{Approach I}
    \begin{algorithm}[H]
        \caption{Solving for $v$ directly}
        \begin{algorithmic}[1]
        \Procedure{MCCALL}{} \
        \State $k \gets 1$, $\e \gets \tau + 1$, $h_k \gets c$
        \While{$\e>\tau$} \
        \State $h_{k+1}\gets  c + \b \sum_{w^\prime \in \text{W}} \max \bc{\frac{w^\prime}{1-\b},h_k} \phi\of{w^\prime}$
        \State $\e \gets \abs{h_{k+1}-h_k}$, $k \gets k+1$
        \EndWhile
        \For{$w \in \text{W}$} \
        \State $v\of{w} \gets \max \bc{\frac{w}{1-\b}, h_k}$
        \EndFor
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
\end{frame}

\begin{frame}{Approach II}
    \begin{itemize}
        \item We know how to solve a finite horizon problem -- backward induction.
        \item Maybe we can get an approximate solution to the infinite horizon problem by considering a finite horizon problem with a very large number of periods?
        \item We will \al{prove} that it actually works.       
\end{itemize}
\end{frame}


\begin{frame}
    \heading{Mathematical detour: Banach's Contraction mapping theorem}
\end{frame}


\begin{frame}{Existence and uniqueness}
    \begin{itemize}
        \item Before solving any problem it is useful to know if there is a solution at all.
        \item We will use a powerful theorem that will help us answer this question. 
        \item First we introduce a concept of a \alb{fixed point}.
        \item Let $U$ be any nonempty set. We call $T$ a self-map on $U$ if $T: U \to U$.
        \item For a self-map $T$ on $U$, we say that a point $u^*\in U$ is a \alb{fixed point} of $T$ if $Tu^* = u^*$.
    \end{itemize}
\end{frame}

\begin{frame}{Fixed point}
    \begin{itemize}
        \item Some examples of fixed points: 
        \begin{itemize}
        \item  $U = \R$, $T\of{u} = 2u + 3$. Then $u^* = -3$ is a fixed point of $T$.
        \item  $U = \bs{0,1}$, $T\of{u} = u$. Then every $u \in U$ is a fixed point of $T$.    
        \item  $U = \R$, $T\of{u} = u + 1$. There is no fixed point of $T$.
        \end{itemize}
        \item \al{Global stability}: a self-map $T$ on $U$ is \al{globally stable} on $U$ if $T$ has a unique fixed point $u^*$ in $U$ and $T^k u \to u^*$ for all $u \in U$. 
    \end{itemize}
\end{frame}

\begin{frame}{Metric space}
    \begin{itemize}
        \item A metric space is a set $U$, together with a metric (distance function) $\rho$, $\rho: U \times U \to \R$, such that for all $u,v,w \in U$:
        \begin{itemize}
            \item $\rho\of{u,v} \geq 0$ (nonnegativity), with equality if and only if $u=v$.
            \item $\rho\of{u,v} = \rho\of{v,u}$ (symmetry).
            \item $\rho\of{u,v} \leq \rho\of{u,w} + \rho\of{w,v}$ (triangle inequality).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Metric space}
    \begin{itemize}
        \item \alg{Convergence}: a sequence $\bc{x_n}^\infty_{n=0}$ in U \alg{converges} to $x \in U$, if for each $\e>0$, there exists $N_\e$ such that $\rho\bp{x_n,x}<\e, \forall n \geq N_\e$.
        \item \alr{Cauchy sequence}: A sequence $\bc{x_n}^\infty_{n=0}$ in $U$ is \alr{Cauchy} if for each $\e>0$, there exists $N_\e$ such that $\rho\bp{x_n,x_m}<\e, \forall n,m \geq N_\e$.
        \item \alb{Complete metric space}: A metric space $\bp{U,\rho}$ is \alb{complete} if every \alr{Cauchy sequence} in $U$ \alg{converges} to an element in $U$.

        \item Example: $\R$ with $\rho\of{u,v} = \abs{u-v}$ is a complete metric space.
    \end{itemize}
\end{frame}

\begin{frame}{Normed vector space}
    \begin{itemize}
        \item A normed vector space is a vector space $V$ over $\R$ or $\C$ together with a norm $\norm{\cdot}$, $\norm{\cdot}: V \to \R$, such that for all $u,v \in V$ and $\a \in \R$ or $\C$: 
        \begin{itemize}
            \item $\norm{u} \geq 0$ (nonnegativity), with equality if and only if $u=0$.
            \item $\norm{\a u} = \abs{\a} \norm{u}$ (absolute homogeneity).
            \item $\norm{u+v} \leq \norm{u} + \norm{v}$ (triangle inequality).
        \end{itemize}
        \item Some norms: $\norm{u}_1 = \sum_{i=1}^n \abs{u_i}$ (Manhattan), $\norm{u}_2 = \bp{\sum_{i=1}^n u_i^2}^{1/2}$ (Euclidean), $\norm{u}_\infty = \max_{i=1,\ldots,n} \abs{u_i}$ (supremum).
       \item We will only focus on real vector spaces. 
       \item A normed vector space is a metric space with $\rho\of{u,v} = \norm{u-v}$.
       \item A complete normed vector space is called a \alb{Banach space}.
    \end{itemize}
\end{frame}

\begin{frame}{Normed vector space}
    \begin{itemize}
        \item Let $X\subseteq \R^n$ be a nonempty set, let $C\of{X}$ be the set of \alr{bounded continuous functions} on $X$ with the supremum norm, $\norm{f}_\infty = \sup_{x\in X} \abs{f\of{x}}$. Then $C$ is a Banach space (complete normed vector space). 
    \end{itemize}
\end{frame}

\begin{frame}{Contraction}
    \begin{itemize}
        \item Let $\bp{U,\rho}$ be a \alb{metric space} and $T$ a self-map on $U$. $T$ is a \alr{contraction mapping} (with modulus $\lambda$) if for some $\l\in\bp{0,1}$ \begin{align*}
            \rho\of{Tu,Tv} \leq \l \rho\of{u,v}, \quad \text{ for all } u,v \in U.
        \end{align*}
        \item If $T$ is a contraction on $U$, then $T$ is uniformly continuous on $U$.
    \end{itemize}
\end{frame}

\begin{frame}{Banach's contraction mapping theorem}

    \begin{theorem}[Banach's contraction mapping theorem]
        If $\bp{U,\rho}$ is a complete metric space and a self-map $T$ is a contraction mapping with modulus $\l$, then:
        \begin{itemize}
            \item $T$ has a unique fixed point $u^*$ in $U$, and 
            \item for any $u_0 \in U$, $\rho\of{T^k u_0,u^*} \leq \l^k \rho\of{u_0,u^*}$ for all $k\in\N$.
        \end{itemize}
        \end{theorem}

\end{frame}

\begin{frame}{Banach's contraction mapping theorem}
\begin{itemize}
    \item By the contraction property of $T$ we have: \begin{align*}
        \rho\of{u_2,u_1} = \rho\of{Tu_1,Tu_0}  \leq \l \rho\of{u_1,u_0}.
    \end{align*}
    By induction: \begin{align*}
        \rho\of{u_{k+1},u_k} \leq \l^k \rho\of{u_1,u_0}, n = 1,2,\ldots.
    \end{align*}
    Using it and the triangle inequality, for $m>n$ \begin{align*}
        \rho\of{u_m,u_n} &\leq \rho\of{u_m,u_{m-1}} + \rho\of{u_{m-1},u_{m-2}} + \cdots + \rho\of{u_{n+1},u_n} \\ &
        \leq \bs{\l^{m-1}+\l^{m-2}+\cdots+\l^n} \rho\of{u_1,u_0} \\
        &\leq  \frac{\l^n}{1-\l} \rho\of{u_1,u_0}.
    \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Proof}
    \begin{itemize}
        \item From \begin{align*}
            \rho\of{u_{k+1},u_k}\leq \frac{\l^k}{1-\l} \rho\of{u_1,u_0}
        \end{align*}
        we see that $\bc{u_k}^\infty_{k=0}$ is a \alr{Cauchy sequence}.
        \item Since $U$ is complete, $\bc{u_k}^\infty_{k=0}$ converges to some $u^* \in U$. 
        \item  We now show that $u^* $ is a \alb{fixed point} of $T$. For all $n$ and all $u_0 \in U$, \begin{align*}
            \rho\of{Tu^* ,u^* } &\leq \rho\of{Tu^* ,T^n u_0} + \rho\of{T^n u_0,u^* } \\
            & \leq \l \rho\of{u^* ,T^{n-1}u_0} + \rho\of{T^n u_0, u^* }.
        \end{align*}
        \item By the previous result both terms on the right hand side go to 0 as $n \to \infty$.           
    \end{itemize}

\end{frame}
\begin{frame}{Proof}
    \begin{itemize}
        \item We need to show there is no other $\hat{u}$ such that $T\hat{u} = \hat{u}$.
        \item Suppose there is such $\hat{u}\neq u^*$. Take $\rho\of{\hat{u},u^*}=\delta>0$ Then \begin{align*}
            \delta = \rho\of{u^* ,\hat{u}} = \rho\of{Tu^* ,T\hat{u}} \leq \l \rho\of{u^* ,\hat{u}} = \l \delta.
        \end{align*}
        but $\delta\leq\delta\l$ \al{cannot} hold because $\l<1$!
    \end{itemize}
    \end{frame}


\begin{frame}{Proof}
    \begin{itemize}
        \item To prove "for any $u_0 \in U$, $\rho\of{T^k u_0,u^*} \leq \l^k \rho\of{u_0,u^*}$ for all $k\in\N$" notice that for any $n\geq1$:
        \begin{align*}
            \rho\of{T^k u_0,u^*} = \rho\of{T\of{T^{k-1} u_0}, Tu^*} \leq \l \rho\of{T^{k-1} u_0,u^*}.
        \end{align*}
        \item We can also show that \begin{align*}
            \rho\of{T^k u_0,u^*} \leq \frac{1}{1-\l} \rho\of{T^k u_0,T^{k-1} u_0}.
        \end{align*}
    \end{itemize}
    \end{frame}

\begin{frame}{Banach's contraction mapping theorem}
    \begin{itemize}
        \item Banach's contraction mapping theorem is a very powerful result.
        \item First, we can use it to \al{show} that a \al{unique solution exists} (fixed point).
        \item Second, it gives us a way to \alb{find} the fixed point -- we can iterate the contraction mapping (successive approximation / fixed point iteration)
        \item It proves that the fixed point iteration converges. It also gives us a bound on the rate of convergence ($\l$, the modulus of contraction).
        \item We often use it to prove the existence of a solution to a \al{functional equation} or to find a \alg{stationary distribution}.
    \end{itemize}
    \end{frame}



    \begin{frame}{An example}
        \begin{itemize}
            \item Let $U = \R$ and $\norm{\cdot} = \abs{\cdot}$, and $T$ be a self-map on $U$ defined by $T\of{u} = 0.5u + 3$.
            \item $U$ with $\rho\of{u,v}=\abs{u-v}$ is a Banach space. 
            \item We have \begin{align*} 
                \rho\of{T\of{u},T\of{v}} &=  \abs{T\of{u}-T\of{v}} \\ &= \abs{0.5u + 3 - 0.5v - 3} \\ &= 0.5 \cdot \abs{u-v} \\ & = 0.5 \cdot \rho\of{u,v}.
            \end{align*} so $T$ is a contraction with modulus $\l=0.5$.
            \item By the CMT, there exists a unique fixed point of $T$ on $U$: $u^* = 6$.
        \end{itemize}
        \end{frame}

        \begin{frame}{An example}
        \begin{itemize}
            \item Another application: Picard-Lindel\"of theorem.
            \item Suppose we have an initial value problem: \begin{align*}
                y^\prime\of{t} = f\of{t,y\of{t}}, \quad y\of{t_0} = y_0.
                \end{align*}
            \item If $f\of{t,\cdot}$ is continuous and bounded and $f\of{t,\cdot}$ is Lipschitz continuous in $y$ with Lipschitz constant $L$ for every $t\in\bs{t_0-\alpha,t_0 +\alpha}$, then there exists a unique solution to the problem in the neighborhood of $t_0$.
        \end{itemize}
        \end{frame}


\begin{frame}
    \heading{Back to job search model}
    \end{frame}
\begin{frame}{Bellman equation}
    \begin{itemize}
        \item We want to solve the Bellman equation:
        \begin{align*}
            v\of{w} = \max \bc{\frac{w}{1-\b}, c + \b \sum_{w^\prime \in \text{W}} v\of{w^\prime} \phi\of{w^\prime}}.
        \end{align*}
        \item Introduce a \alb{Bellman operator} defined at $v \in \R^W$ as \begin{align*}
            \bp{Tv}\of{w} \coloneq \max \bc{\frac{w}{1-\b}, c + \b \sum_{w^\prime \in \text{W}} v\of{w^\prime} \phi\of{w^\prime}}.
            \end{align*}
        \item Note: here we treat $v$ as a vector, not a function. We can do it, because $\text{W}$ is finite.
        \item Let $V \coloneq \R^W_+$ and let $\norm{\cdot}_\infty$ be the supremum norm. Note that $V$ with this norm is a Banach space.
    \end{itemize}
    \end{frame}
 
\begin{frame}{Bellman equation}
    \begin{itemize}
        \item Notice that \begin{align*}
            \abs{\max\bc{a,x} - \max\bc{a,y}} \leq \abs{x-y} \quad \text{for all } a,x,y \in \R.
        \end{align*}
        \item Take any $f,g\in V$ and fix any $w\in\text{W}$. Use the above to get 
        \begin{align*}
            \abs{\bp{Tf}\of{w} - \bp{Tg}\of{w}}  & \leq \b\abs{\sum_{w^\prime \in \text{W}} \bs{f\of{w^\prime}-g\of{w^\prime}}\phi\of{w^\prime}} \\ 
            & \leq \b \norm{f-g}_\infty
        \end{align*}
        \item Take the supremum over $w$ to get \begin{align*}
            \norm{Tf-Tg}_\infty \leq \b \norm{f-g}_\infty.
        \end{align*}
        \item This proves $T$ is a contraction with modulus $\b$.
    \end{itemize}
    \end{frame}


\begin{frame}{Bellman equation}
    \begin{itemize}
        \item There exists a unique fixed point $v^*$ of $T$ in $V$. 
        \item The fixed point of the Bellman operator solves the Bellman equation. 
        \item The solution to the Bellman equation is a fixed point of the Bellman operator.
        \item We can obtain $v^*$ by iterating the Bellman operator: \begin{align*}
            v_{k+1} = Tv_k, \quad k=0,1,\ldots.
        \end{align*}
        \item We can start with \al{any} $v_0 \in V$.
    \end{itemize}
    \end{frame}

\begin{frame}{Approach II}
    \begin{algorithm}[H]
        \caption{Value function iteration}
        \begin{algorithmic}[1]
        \Procedure{VFI}{} \
        \State $k \gets 1$, $\e \gets \tau + 1$, $v_k \gets v_{init}$
        \While{$\e>\tau$} \
        \For{$w \in \text{W}$} \
        \State $v_{k+1}\of{w} \gets \bp{Tv_{k}}\of{w}$
        \EndFor
        \State $\e \gets \norm{v_{k+1}-v_k}_\infty$, $k \gets k+1$
        \EndWhile
        
        \EndProcedure
        \end{algorithmic}
        \end{algorithm}
\end{frame}

\begin{frame}{Optimal choices}
    \begin{itemize}
        \item Once we have $v^*$ we can characterize the optimal choice of the agent.
        \item We can calculate the continuation value $h^*$: \begin{align*}
            h^* \coloneq c + \b \sum_{w^\prime \in \text{W}} v^*\of{w^\prime}\phi\of{w^\prime}. \end{align*}
        \item Reject the offer if $w/\bp{1-\b} < h^*$, accept otherwise. 
        \item Denote rejection given wage $W_t$ as $A_t = 0$ and acceptance as $A_t = 1$.

    \end{itemize}
\end{frame}

\begin{frame}{Optimal choices}
    \begin{itemize}
        \item Let $A_t =  \sigma_t \of{W_t}$ be the optimal choice of the agent at time $t$ given wage offer $W_t$.
        \item We call $\sigma_t$ the (time t) \alb{policy function}.
        \item In this particular case the policy function is \begin{align*}
            \sigma_t\of{w} = \begin{cases}
                1 & \text{if } \frac{w}{1-\b} \geq h^* \\
                0 & \text{otherwise}.
            \end{cases}
        \end{align*}
        \item The policy function here depends only on the current state (wage). 
        \item We call such a policy function (depending on the current state only) \alb{Markov policy}.
    \end{itemize}
\end{frame}

\begin{frame}{Optimal choices}
    \begin{itemize}
        \item A policy is an "instruction manual" for the agent: what to do in each state.
        \item For an agent following $\sigma\in\Sigma$, if the current wage offer is $w$, the agent will responds with $\sigma\of{w}\in\bc{0,1}$.
        \item For each $v\in V$, a \alr{$v-\text{greedy}$ policy} is a $\sigma\in\Sigma$ satisfying
        \begin{align*}
            \sigma\of{w} = \mathbb{1} \bc{\frac{w}{1-\b}\geq c + \b \sum_{w^\prime \in \text{W}} v\of{w^\prime} \phi\of{w^\prime}} \quad \text{for all } w\in \text{W}.
        \end{align*}
        \item The recommendation is: \al{adopt a $v^*-\text{greedy}$ policy} (notice the superscript!).
        \item This is a restatement of \alb{Bellman's principle of optimality}.
    \end{itemize}
\end{frame}

\begin{frame}{Looking forward}
    \begin{itemize}
        \item Here we have a \al{finite} state space -- we can treat $v$ as a vector.
        \item We only used a fraction of the power of dynamic programming...
        \item What if we move \alr{away} from finite state and action spaces?
        \item What are the conditions under which the Bellman operator is a \alg{contraction}? Is there an easy way to check it?
        \item Does the \alb{principle of optimality} hold in \alb{general}?
    \end{itemize}
\end{frame}

\end{document}